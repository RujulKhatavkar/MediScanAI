{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from mlx_lm import load, generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 37259.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <s>[INST] MedBotAI functions as a virtual assistant, guiding users through the analysis and understanding of medical questionnaire datasets. It communicates in clear, accessible language, ensuring that users feel informed and supported. MedBotAI responds to medical inquiries with accuracy and empathy, offering deeper explanations upon request. When providing advice or insights based on the dataset, it uses scientifically-backed information and always encourages consulting healthcare professionals for final decisions. MedBotAI will adjust its response length based on the complexity of the user's question, giving concise answers for simple queries and detailed explanations for complex ones.\n",
      "\n",
      "Please respond to the following question about the medical dataset.\n",
      " \n",
      "Is Adderall XR harmful? \n",
      "[/INST]\n",
      "\n",
      "MedBotAI: I'd be happy to help answer your question about Adderall XR, but it's important to note that I can't provide definitive answers regarding the harm or safety of any specific medication without considering individual health circumstances. Adderall XR is a prescription medication used primarily to treat attention deficit hyperactivity disorder (ADHD) and narcolepsy. It contains a time-released formulation of amphetamine and dextroamphetamine.\n",
      "\n",
      "Like all medications, Adderall XR can have side effects, and some people may experience adverse reactions. Common side effects include:\n",
      "\n",
      "1. Dry mouth\n",
      "2. Stomach upset\n",
      "3. Loss of appetite\n",
      "4. Weight loss\n",
      "5. Insomnia\n",
      "6. Increased heart rate\n",
      "7. Increased blood pressure\n",
      "8. Dizziness\n",
      "9. Headache\n",
      "10. Nervousness or anxiety\n",
      "11\n",
      "==========\n",
      "Prompt: 162 tokens, 52.069 tokens-per-sec\n",
      "Generation: 200 tokens, 6.444 tokens-per-sec\n",
      "Peak memory: 4.071 GB\n"
     ]
    }
   ],
   "source": [
    "model_path = \"mlx-community/Mistral-7B-Instruct-v0.2-4bit\"\n",
    "prompt = prompt_builder(\"Is Adderall XR harmful?\")\n",
    "max_tokens = 200\n",
    "\n",
    "# load model\n",
    "model, tokenizer = load(model_path)\n",
    "\n",
    "# generate response\n",
    "response = generate(model, tokenizer, prompt=prompt, \n",
    "                                      max_tokens = max_tokens, \n",
    "                                      verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_command_with_live_output(command: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Courtesy of ChatGPT:\n",
    "    Runs a command and prints its output line by line as it executes.\n",
    "\n",
    "    Args:\n",
    "        command (List[str]): The command and its arguments to be executed.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    # Print the output line by line\n",
    "    while True:\n",
    "        output = process.stdout.readline()\n",
    "        if output == '' and process.poll() is not None:\n",
    "            break\n",
    "        if output:\n",
    "            print(output.strip())\n",
    "        \n",
    "    # Print the error output, if any\n",
    "    err_output = process.stderr.read()\n",
    "    if err_output:\n",
    "        print(err_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_shell_command(command: list[str]) -> str:\n",
    "    \n",
    "    return str(command).replace(\"'\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\",\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions_string = f\"\"\"MedBotAI functions as a virtual assistant, guiding users through the analysis and understanding of medical questionnaire datasets. \\\n",
    "It communicates in clear, accessible language, ensuring that users feel informed and supported. \\\n",
    "MedBotAI responds to medical inquiries with accuracy and empathy, offering deeper explanations upon request. \\\n",
    "When providing advice or insights based on the dataset, it uses scientifically-backed information. \\\n",
    "MedBotAI will adjust its response length based on the complexity of the user's question, giving concise answers for simple queries and detailed explanations for complex ones.\n",
    "\n",
    "Please respond to the following question about the medical dataset.\n",
    "\"\"\"\n",
    "\n",
    "prompt_builder = lambda question: f'''<s>[INST] {instructions_string} \\n{question} \\n[/INST]\\n'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = \"adapters.npz\" # same as default\n",
    "max_tokens_str = str(max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rujul/Documents/Projects/OCR/myenv/bin/python: can't open file '/Users/rujul/Documents/Projects/OCR/scripts/lora.py': [Errno 2] No such file or directory\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# define command\n",
    "command = ['python', 'scripts/lora.py', '--model', model_path, '--adapter-file', adapter_path, '--max-tokens', max_tokens_str, '--prompt', prompt]\n",
    "\n",
    "# run command and print results continuously\n",
    "run_command_with_live_output(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"mlx-community/Mistral-7B-Instruct-v0.2-4bit\"\n",
    "prompt = prompt_builder(\"Is Adderall XR? harmful\")\n",
    "max_tokens = 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 80438.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <s>[INST] MedBotAI functions as a virtual assistant, guiding users through the analysis and understanding of medical questionnaire datasets. It communicates in clear, accessible language, ensuring that users feel informed and supported. MedBotAI responds to medical inquiries with accuracy and empathy, offering deeper explanations upon request. When providing advice or insights based on the dataset, it uses scientifically-backed information and always encourages consulting healthcare professionals for final decisions. MedBotAI will adjust its response length based on the complexity of the user's question, giving concise answers for simple queries and detailed explanations for complex ones.\n",
      "\n",
      "Please respond to the following question about the medical dataset.\n",
      " \n",
      "Is Adderall XR? harmful \n",
      "[/INST]\n",
      "\n",
      "MedBotAI: I'd be happy to help answer your question about Adderall XR. Adderall XR is a prescription medication that contains a combination of amphetamine and dextroamphetamine. It's primarily used to treat attention deficit hyperactivity disorder (ADHD) and narcolepsy. The \"XR\" in Adderall XR stands for extended-release, which means the medication is designed to provide a consistent level of the drug in the body for a longer period of time.\n",
      "\n",
      "Regarding your question, it's essential to clarify that I cannot definitively determine if Adderall XR is harmful to you without\n",
      "==========\n",
      "Prompt: 162 tokens, 46.596 tokens-per-sec\n",
      "Generation: 140 tokens, 6.781 tokens-per-sec\n",
      "Peak memory: 7.939 GB\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load(\"mlx-community/Mistral-7B-Instruct-v0.2-4bit\")\n",
    "response = generate(model, tokenizer, prompt=prompt, max_tokens = max_tokens,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split completed: 3404 training samples, 730 validation samples, 730 test samples.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_dataset(json_file, train_ratio=0.7, valid_ratio=0.15, test_ratio=0.15):\n",
    "    # Step 1: Load the data from the JSON file\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Ensure the ratios add up to 1\n",
    "    assert train_ratio + valid_ratio + test_ratio == 1, \"The sum of the ratios must be 1.\"\n",
    "\n",
    "    # Step 2: Shuffle and split the data\n",
    "    train_data, temp_data = train_test_split(data, test_size=(valid_ratio + test_ratio))\n",
    "    valid_data, test_data = train_test_split(temp_data, test_size=test_ratio / (valid_ratio + test_ratio))\n",
    "\n",
    "    # Step 3: Save the data into separate files\n",
    "    with open('data/train.jsonl', 'w') as train_file:\n",
    "        json.dump(train_data, train_file, indent=4)\n",
    "\n",
    "    with open('data/valid.jsonl', 'w') as valid_file:\n",
    "        json.dump(valid_data, valid_file, indent=4)\n",
    "\n",
    "    with open('data/test.jsonl', 'w') as test_file:\n",
    "        json.dump(test_data, test_file, indent=4)\n",
    "\n",
    "    print(f\"Data split completed: {len(train_data)} training samples, {len(valid_data)} validation samples, {len(test_data)} test samples.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'train.json' with the actual path to your file\n",
    "    split_dataset('train1.json', train_ratio=0.7, valid_ratio=0.15, test_ratio=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split completed: 3404 training samples, 730 validation samples, 730 test samples.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_dataset(json_file, train_ratio=0.7, valid_ratio=0.15, test_ratio=0.15):\n",
    "    # Step 1: Load the data from the JSON file\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Ensure the ratios add up to 1\n",
    "    assert train_ratio + valid_ratio + test_ratio == 1, \"The sum of the ratios must be 1.\"\n",
    "\n",
    "    # Step 2: Shuffle and split the data\n",
    "    train_data, temp_data = train_test_split(data, test_size=(valid_ratio + test_ratio))\n",
    "    valid_data, test_data = train_test_split(temp_data, test_size=test_ratio / (valid_ratio + test_ratio))\n",
    "\n",
    "    # Step 3: Save the data into separate files in jsonl format\n",
    "    def save_as_jsonl(filename, data):\n",
    "        with open(filename, 'w') as f:\n",
    "            for entry in data:\n",
    "                f.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "    # Save datasets as jsonl\n",
    "    save_as_jsonl('data/train.jsonl', train_data)\n",
    "    save_as_jsonl('data/valid.jsonl', valid_data)\n",
    "    save_as_jsonl('data/test.jsonl', test_data)\n",
    "\n",
    "    print(f\"Data split completed: {len(train_data)} training samples, {len(valid_data)} validation samples, {len(test_data)} test samples.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'train1.json' with the actual path to your file\n",
    "    split_dataset('train1.json', train_ratio=0.7, valid_ratio=0.15, test_ratio=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = \"Explain side effects of Topamax\"\n",
    "prompt = prompt_builder(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n",
      "Total parameters 1243.189M\n",
      "Trainable parameters 0.852M\n",
      "Loading datasets\n",
      "Generating\n",
      "<s>[INST] MedBotAI functions as a virtual assistant, guiding users through the analysis and understanding of medical questionnaire datasets. It communicates in clear, accessible language, ensuring that users feel informed and supported. MedBotAI responds to medical inquiries with accuracy and empathy, offering deeper explanations upon request. When providing advice or insights based on the dataset, it uses scientifically-backed information and always encourages consulting healthcare professionals for final decisions. MedBotAI will adjust its response length based on the complexity of the user's question, giving concise answers for simple queries and detailed explanations for complex ones.\n",
      "\n",
      "Please respond to the following question about the medical dataset.\n",
      "\n",
      "Explain side effects of Topamax\n",
      "[/INST]\n",
      "ShawGPT responds:\n",
      "Side effects of Topamax may occur when the body gets used to this medication. Not everyone will experience side effects, but some people do. Side effects of Topamax can include:\n",
      "1. Dizziness, drowsiness, or trouble concentrating\n",
      "2. Trouble sleeping or staying awake, or feeling tired during the day\n",
      "3. Appetite or weight changes\n",
      "4. Hair loss, or thinning of the hair at the temples and crown area\n",
      "5. Blurred vision or double vision\n",
      "6. Tingling or numbness in the hands or feet\n",
      "7. Diarrhea or stomach pain or cramps\n",
      "8. Weight loss or gain\n",
      "9. Numbness or tingling in the face\n",
      "10. Cough, wheezing, or shortness of breath\n",
      "11. Eye pain or redness, or vision problems\n",
      "12. Weakness\n",
      "==========\n",
      "/Users/rujul/Documents/Projects/OCR/myenv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "\n",
      "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 115137.76it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define command\n",
    "command = ['python', 'lora.py', '--model', model_path, '--adapter-file', adapter_path, '--max-tokens', max_tokens_str, '--prompt', prompt]\n",
    "\n",
    "# run command and print results continuously\n",
    "run_command_with_live_output(command)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
